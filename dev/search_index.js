var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Modules = [OnlineStats, OnlineStatsBase]\nFilter = T -> typeof(T) === DataType ? !(T <: OnlineStats.Weight) : true","category":"page"},{"location":"api/#OnlineStats.ADADELTA","page":"API","title":"OnlineStats.ADADELTA","text":"ADADELTA(ρ = .95)\n\nAn extension of ADAGRAD.\n\nReference: https://ruder.io/optimizing-gradient-descent/\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.ADAGRAD","page":"API","title":"OnlineStats.ADAGRAD","text":"ADAGRAD()\n\nA variation of SGD with element-wise weights generated by the average of the squared gradients.\n\nReference: https://ruder.io/optimizing-gradient-descent/\nNote: ADAGRAD uses a learning rate in OnlineStats, which differs from how it is typically presented. See https://www.seqstat.com/post/adagrad/ for details.\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.ADAM","page":"API","title":"OnlineStats.ADAM","text":"ADAM(β1 = .99, β2 = .999)\n\nA variant of SGD with element-wise learning rates generated by exponentially weighted first and second moments of the gradient.\n\nReference: https://ruder.io/optimizing-gradient-descent/\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.ADAMAX","page":"API","title":"OnlineStats.ADAMAX","text":"ADAMAX(η, β1 = .9, β2 = .999)\n\nADAMAX with momentum parameters β1, β2.  ADAMAX is an extension of ADAM.\n\nReference: https://ruder.io/optimizing-gradient-descent/\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.Ash","page":"API","title":"OnlineStats.Ash","text":"Ash(h::Union{KHist, Hist, ExpandingHist})\nAsh(h::Union{KHist, Hist, ExpandingHist}, m::Int, kernel::Function)\n\nCreate an Average Shifted Histogram using h as the base histogram with smoothing parameter m and kernel function kernel.  Built-in kernels are available in the OnlineStats.Kernels module.\n\nExample\n\nusing OnlineStats, Plots\n\no = fit!(Ash(ExpandingHist(1000)), randn(10^6))\n\nplot(o)\nplot(o, 20)\nplot(o, OnlineStats.Kernels.epanechnikov, 4)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.AutoCov","page":"API","title":"OnlineStats.AutoCov","text":"AutoCov(b, T = Float64; weight=EqualWeight())\n\nCalculate the auto-covariance/correlation for lags 0 to b for a data stream of type T.\n\nExample\n\ny = cumsum(randn(100))\no = AutoCov(5)\nfit!(o, y)\nautocov(o)\nautocor(o)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.BiasVec","page":"API","title":"OnlineStats.BiasVec","text":"BiasVec(x)\n\nLightweight wrapper of a vector which adds a bias/intercept term at the end.\n\nExample\n\nBiasVec(rand(5))\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.Bootstrap","page":"API","title":"OnlineStats.Bootstrap","text":"Bootstrap(o::OnlineStat, nreps = 100, d = [0, 2])\n\nCalculate an online statistical bootstrap of nreps replicates of o.  For each call to fit!, any given replicate will be updated rand(d) times (default is double or nothing).\n\nExample\n\no = Bootstrap(Variance())\nfit!(o, randn(1000))\nconfint(o, .95)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.CCIPCA","page":"API","title":"OnlineStats.CCIPCA","text":"CCIPCA(outdim::Int, indim; l::Int)\n\nOnline PCA with the CCIPCA (Candid Covariance-free Incremental PCA) algorithm, where indim is the length of incoming vectors, outdim is the number of dimension to project to, and l is the level of amnesia. Give values of l in the range 2-4 if you want old vectors to be gradually less important, i.e. latest vectors added get more weight.\n\nIf no indim is specified it will be set later, on first call to fit. After that it is fixed and cannot change.\n\nThe CCIPCA is a very fast, simple, and online approximation of PCA. It can be used for Dimensionality Reduction to project high-dimensional vectors into a low-dimensional (typically 2D or 3D) space. This algorithm has shown very good properties in comparative studies; it is both fast and give a good approximation to (batch) PCA.\n\nExample\n\no = CCIPCA(2, 10)                # Project 10-dimensional vectors into 2D\nu1 = rand(10)\nfit!(o, u1)                      # Fit to u1\nu2 = rand(10)\nfit!(o, u2)                      # Fit to u2\nu3 = rand(10)\nOnlineStats.transform(o, u3)     # Project u3 into PCA space fitted to u1 and u2 but don't change the projection\nu4 = rand(10)\nOnlineStats.fittransform!(o, u4) # Fit u4 and then project u4 into the space\nsort!(o)                         # Sort from high to low eigenvalues\no[1]                             # Get primary (1st) eigenvector\nOnlineStats.relativevariances(o)         # Get the variation (explained) \"by\" each eigenvector\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.CallFun","page":"API","title":"OnlineStats.CallFun","text":"CallFun(o::OnlineStat, f::Function)\n\nCall f(o) every time the OnlineStat o gets updated.\n\nExample\n\no = CallFun(Mean(), println)\nfit!(o, [0,0,1,1])\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.Cluster","page":"API","title":"OnlineStats.Cluster","text":"Cluster center and the number of observations\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.CountMinSketch","page":"API","title":"OnlineStats.CountMinSketch","text":"CountMinSketch(T = Number; nhash=50, hashlength=1000, storagetype=Int)\n\nCreate a Count-Min Sketch with nhash hash functions of width hashlength.  Counts in the sketch will be stored as storagetype.\n\nThis lets you calculate an upper bound on the number of occurrences of some value x in a data stream via value(count_min_sketch, x).  For details and error bounds, please see the references section.\n\nExample\n\no = CountMinSketch()\n\ny = rand(1:100, 10^6)\n\nfit!(o, y)\n\nvalue(o, 1)\n\nsum(y .== 1)\n\nReferences\n\nhttps://florian.github.io/count-min-sketch/\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.DPMM","page":"API","title":"OnlineStats.DPMM","text":"DPMM(comp_mu::Real,\n     comp_lambda::Real,\n     comp_alpha::Real,\n     comp_beta::Real,\n     dirichlet_alpha::Real;\n     comp_birth_thres=1e-2,\n     comp_death_thres=1e-2,\n     n_comp_max=10)\n\nOnline univariate dirichlet process Gaussian mixture model algorithm.\n\nMathematical Description\n\nThe model is described as\n\nG ~ DP(dirchlet_alpha, Normal-Gamma)\n(μₖ, τₖ) ~ G\nx ~ N(μ, 1/sqrt(τ))\n\nwhere the base measure is defined as\n\nτₖ ~ Gamma(comp_alpha, 1/comp_beta)\nμₖ ~ N(comp_mu, 1/sqrt(comp_lambda*τₖ)).\n\nThe variational distribution is the mean-field family defined as\n\nq(μₖ | τₖ; mₖ, lₖ) q(τₖ; aₖ, bₖ) = N(μₖ; mₖ, 1/(lₖ*τₖ)) Gamma(τₖ; aₖ, 1/bₖ).\n\nSince the model is nonparametric, mixture components are added depending on the birth threshold comp_birth_thres (higher means less frequen births) and  existing components are pruned depending on the death threshold comp_death_thres.\n\nHyperparameters\n\nDPMMs tend to be very sensitive to its hyperparameters. Therefore, it is important to monitor the fitted result and tweak the hyperparameters accordingly. Here are the implications of each hyperparameter:\n\ncomp_mu: Prior mean of the components \ncomp_lambda: Prior precision of the components. This affects the dispersion                of the components relative to the scale of each component.                (Smaller the more dispersed.) (comp_lambda > 0)\ncomp_alpha: Gamma shape parameter of the scale (inverse of the variance) of               each component. (comp_alpha > 0)\ncomp_beta: Gamma scale parameter of the scale (inverse of the variance) of              each component. (comp_beta > 0)\ndirichlet_alpha: Initial weight of a newly added component. Larger values                    result in more components being created.                    (dirichlet_alpha > 0)\ncomp_birth_thres: Threshold for adding a new component (highly affected by                     dirichlet_alpha)\ncomp_death_thres: Threshold for prunning (or killing) an existing component.\n\nA mechanical procedure for setting the component hyperparameters is to first set comp_alpha > 2. This ensures the variance to be finite. Then, solve comp_beta such that the Gamma distribution concentrates most of the probability density on the expected range of 1/τₖ, the variance of each component. Finally, solve for comp_lambda such that the marginal density of μₖ, which is a Student-T distribution\n\np(μₖ | λ₀, α₀, β₀)\n    = ∫ p(μₖ | τₖ, μ₀, λ₀) p(τₖ | α₀, β₀) dτₖ\n    = ∫ N(μₖ; μ₀, 1/sqrt(λ₀*τₖ)) Gamma(τₖ; αₖ, βₖ) dτₖ\n    = TDist( 2*α₀, μ₀, sqrt(β₀/(λ₀*α₀)),\n\nhas its probability density concentrated on the expected range of the component means.\n\nBelow is a implementing the prior elicitation procedure described above:\n\nusing Roots\n\nprob_τₖ = 0.8\nτₖ_max  = 0.5\nμ₀      = 0.0\nα₀      = 2.1\nprob_μₖ = 0.8\nμₖ_min  = -2\nμₖ_max  = 2\n\nβ₀ = find_zero(β₀ -> cdf(InverseGamma(α₀, β₀), τₖ_max) - prob_τₖ, (1e-4, Inf))\nλ₀ = find_zero(λ₀ -> begin\n    p_μ = TDist(2*α₀)*sqrt(β₀/(λ₀*α₀)) + μ₀\n    cdf(p_μ, μₖ_max) - cdf(p_μ, μₖ_min) - prob_τₖ\nend, (1e-4, 1e+2))\n\nprob_* sets the amount of density on the desired range. τₖ_max, μₖ_min, μₖ_max is the expected range of each parameters. Since we leave 1 - prob_* density on the tails, these are soft contraints.\n\nUnfortunately, dirichlet_alpha, comp_birth_thres, comp_death_thres can be tuned only through trial and error. However, dirichlet_alpha is best set 0.5 ~ 1e-2 depending on the desired number of components.\n\nExample\n\nn    = 1024\nμ    = 0.0\nλ    = 0.1\nα    = 2.1\nβ    = 0.5\nα_dp = 1.0\no    = DPMM(μ, λ, α, β, α_dp; comp_birth_thres=0.5,\n            comp_death_thres=1e-2, n_comp_max=10) \np = MixtureModel([ Normal(-2.0, 0.5), Normal(3.0, 1.0) ], [0.7, 0.3])\no = fit!(o, rand(p, 1000))\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.DWDLoss","page":"API","title":"OnlineStats.DWDLoss","text":"DWDLoss(q)(y, xβ)\n\nDistance-weighted discrimination loss function (smoothed l1hingeloss) with smoothing parameter q.  Loss is calculated between a boolean response y ∈ {-1, 1} and linear predictor xβ.\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.Diff","page":"API","title":"OnlineStats.Diff","text":"Diff(T::Type = Float64)\n\nTrack the difference and the last value.\n\nExample\n\no = Diff()\nfit!(o, [1.0, 2.0])\nlast(o)\ndiff(o)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.ElasticNet","page":"API","title":"OnlineStats.ElasticNet","text":"ElasticNet(α)\n\nWeighted average of Ridge (abs) and LASSO (abs2) penalty functions.\n\n(1 - α) * Ridge + α * LASSO\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.ExpandingHist","page":"API","title":"OnlineStats.ExpandingHist","text":"ExpandingHist(nbins)\n\nAn adaptive histogram where the bin edges keep doubling in size in order to contain every observation. nbins must be an even number.  Bins are left-closed and the rightmost bin is closed, e.g.\n\n[a, b), [b, c), [c, d]\n\nExample\n\no = fit!(ExpandingHist(200), randn(10^6))\n\nusing Plots\nplot(o)\n\nDetails\n\nHow ExpandingHist works is best understood through example.  Suppose we start with a histogram of edges/counts as follows:\n\n|1|2|5|3|2|\n\nNow we observe a data point that is not contained in the bin edges:\n\n|1|2|5|3|2|       *\n\nIn order to contain the point, the range of the edges doubles in the direction of the new data point and adjacent bins merge their counts:\n\n|1|2|5|3|2|       *\n \\ / \\ / \\ /      ↓\n  ↓   ↓   ↓       ↓\n| 3 | 8 | 2 | 0 | 1 |\n\nNote that multiple iterations of bin-doubling may occur until the new point is contained by the bin edges.\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.FastForest","page":"API","title":"OnlineStats.FastForest","text":"FastForest(p, nkeys=2; stat=FitNormal(), kw...)\n\nCalculate a random forest where each variable is summarized by stat.\n\nKeyword Arguments\n\nnt=100: Number of trees in the forest\nb=floor(Int, sqrt(p)): Number of random features for each tree to receive\nmaxsize=1000: Maximum size for any tree in the forest\nsplitsize=5000: Number of observations in any given node before splitting\nλ = 5 * (1 / nt): Probability that each tree is updated on a new observation\n\nExample\n\nx, y = randn(10^5, 10), rand(1:2, 10^5)\n\no = fit!(FastForest(10), zip(eachrow(x),y))\n\nclassify(o, x[1,:])\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.FastTree","page":"API","title":"OnlineStats.FastTree","text":"FastTree(p::Int, nclasses=2; stat=FitNormal(), maxsize=5000, splitsize=1000)\n\nCalculate a decision tree of p predictors variables and classes 1, 2, …, nclasses. Nodes split when they reach splitsize observations until maxsize nodes are in the tree. Each variable is summarized by stat, which can be FitNormal() or Hist(nbins).\n\nExample\n\nx = randn(10^5, 10)\ny = rand([1,2], 10^5)\n\no = fit!(FastTree(10), zip(eachrow(x),y))\n\nxi = randn(10)\nclassify(o, xi)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.FitBeta","page":"API","title":"OnlineStats.FitBeta","text":"FitBeta(; weight)\n\nOnline parameter estimate of a Beta distribution (Method of Moments).\n\nExample\n\no = fit!(FitBeta(), rand(1000))\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.FitCauchy","page":"API","title":"OnlineStats.FitCauchy","text":"FitCauchy(b=500)\n\nApproximate parameter estimation of a Cauchy distribution.  Estimates are based on approximated quantiles.  See Quantile and ExpandingHist for details on how the distribution is estimated.\n\nExample\n\no = fit!(FitCauchy(), randn(1000))\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.FitGamma","page":"API","title":"OnlineStats.FitGamma","text":"FitGamma(; weight)\n\nOnline parameter estimate of a Gamma distribution (Method of Moments).\n\nExample\n\nusing Random\no = fit!(FitGamma(), randexp(10^5))\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.FitLogNormal","page":"API","title":"OnlineStats.FitLogNormal","text":"FitLogNormal()\n\nOnline parameter estimate of a LogNormal distribution (MLE).\n\nExample\n\no = fit!(FitLogNormal(), exp.(randn(10^5)))\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.FitMultinomial","page":"API","title":"OnlineStats.FitMultinomial","text":"FitMultinomial(p)\n\nOnline parameter estimate of a Multinomial distribution.  The sum of counts does not need to be consistent across observations.  Therefore, the n parameter of the Multinomial distribution is returned as 1.\n\nExample\n\nx = [1 2 3; 4 8 12]\nfit!(FitMultinomial(3), x)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.FitMvNormal","page":"API","title":"OnlineStats.FitMvNormal","text":"FitMvNormal(d)\n\nOnline parameter estimate of a d-dimensional MvNormal distribution (MLE).\n\nExample\n\ny = randn(100, 2)\no = fit!(FitMvNormal(2), eachrow(y))\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.FitNormal","page":"API","title":"OnlineStats.FitNormal","text":"FitNormal()\n\nCalculate the parameters of a normal distribution via maximum likelihood.\n\nExample\n\no = fit!(FitNormal(), randn(1000))\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.GeometricMean","page":"API","title":"OnlineStats.GeometricMean","text":"GeometricMean(T = Float64)\n\nCalculate the geometric mean of a data stream, stored as type T.\n\nExample\n\no = fit!(GeometricMean(), 1:100)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.HeatMap","page":"API","title":"OnlineStats.HeatMap","text":"Heatmap(xedges, yedges; left = true, closed = true)\nHeatmap(itr; left = true, closed = true)\n\nCreate a two dimensional histogram with the bin partition created by xedges and yedges. When fitting a new observation, the first value will be associated with X, the second with Y.\n\nIf left, the bins will be left-closed.\nIf closed, the bins on the ends will be closed.  See Hist.\n\nExample\n\nusing Plots\n\nxy = zip(randn(10^6), randn(10^6))\no = fit!(HeatMap(-5:.1:5, -5:.1:5), xy)\nplot(o)\n\nxy = zip(1 .+ randn(10^6) ./ 10, randn(10^6))\no = HeatMap(xy)\nplot(o, marginals=false)\nplot(o)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.Hist","page":"API","title":"OnlineStats.Hist","text":"Hist(edges; left = true, closed = true)\n\nCreate a histogram with bin partition defined by edges.\n\nIf left, the bins will be left-closed.\nIf closed, the bin on the end will be closed.\nE.g. for a two bin histogram a b) b c) vs. a b) b c\n\nExample\n\no = fit!(Hist(-5:.1:5), randn(10^6))\n\n# approximate statistics\nusing Statistics\n\nmean(o)\nvar(o)\nstd(o)\nquantile(o)\nmedian(o)\nextrema(o)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.HyperLogLog","page":"API","title":"OnlineStats.HyperLogLog","text":"HyperLogLog(T = Number)\nHyperLogLog{P}(T = Number)\n\nApproximate count of distinct elements of a data stream of type T, using 2 ^ P \"registers\".  P must be an integer between 4 and 16 (default).\n\nBy default it returns the improved HyperLogLog cardinality estimator as defined by [1].\n\nThe original HyperLogLog estimator [2] can be retrieved with the option original_estimator=true.\n\nExample\n\no = HyperLogLog()\nfit!(o, rand(1:100, 10^6))\n\nusing Random\no2 = HyperLogLog(String)\nfit!(o2, [randstring(20) for i in 1:1000])\n\n# by default the improved estimator is returned:\nvalue(o)\n# the original HyperLogLog estimator can be retrieved via:\nvalue(o; original_estimator=true)\n\nReferences\n\n[1] Improved estimator: Otmar Ertl. New cardinality estimation algorithms for HyperLogLog sketches. https://arxiv.org/abs/1702.01284\n\n[2] Original estimator: P. Flajolet, Éric Fusy, O. Gandouet, and F. Meunier. Hyperloglog: The analysis of a near-optimal cardinality estimation algorithm. In Analysis of Algorithms (AOFA), pages 127–146, 2007.\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.IndexedPartition","page":"API","title":"OnlineStats.IndexedPartition","text":"IndexedPartition(T, stat, b=100)\n\nSummarize data with stat over a partition of size b where the data is indexed by a variable of type T.\n\nExample\n\nx, y = randn(10^5), randn(10^6)\no = IndexedPartition(Float64, KHist(10))\nfit!(o, zip(x, y))\n\nusing Plots\nplot(o)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.KHist","page":"API","title":"OnlineStats.KHist","text":"KHist(k::Int)\n\nEstimate the probability density of a univariate distribution at k approximately equally-spaced points.\n\nRef: https://www.jmlr.org/papers/volume11/ben-haim10a/ben-haim10a.pdf\n\nA difference from the above reference is that the minimum and maximum values are not allowed to merge into another bin.\n\nExample\n\no = fit!(KHist(25), randn(10^6))\n\n# Approximate statistics\nusing Statistics\nmean(o)\nvar(o)\nstd(o)\nquantile(o)\nmedian(o)\n\nusing Plots\nplot(o)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.KHist2D","page":"API","title":"OnlineStats.KHist2D","text":"KHist2D(b=300)\n\nApproximate scatterplot of b centers.  This implementation is slow!  See IndexedPartition and KIndexedPartition instead.\n\nExample\n\nx = randn(10^4)\ny = x + randn(10^4)\nplot(fit!(KHist2D(), zip(x, y)))\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.KIndexedPartition","page":"API","title":"OnlineStats.KIndexedPartition","text":"KIndexedPartition(T, stat_init, k=100)\n\nSimilar to IndexedPartition, but indexes the first variable by centroids (like KHist) rather than intervals.\n\nNote: stat_init must be a function e.g. () -> Mean()\n\nExample\n\nusing Plots\n\no = KIndexedPartition(Float64, () -> KHist(10))\n\nfit!(o, zip(randn(10^6), randn(10^6)))\n\nplot(o)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.KMeans","page":"API","title":"OnlineStats.KMeans","text":"KMeans(k; rate=LearningRate(.6))\n\nApproximate K-Means clustering of k clusters.\n\nExample\n\nx = [randn() + 5i for i in rand(Bool, 10^6), j in 1:2]\n\no = fit!(KMeans(2, 2), eachrow(x))\n\nsort!(o; rev=true)  # Order clusters by number of observations\n\nclassify(o, x[1])  # returns index of cluster closest to x[1]\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.KahanMean","page":"API","title":"OnlineStats.KahanMean","text":"KahanMean(; T=Float64, weight=EqualWeight())\n\nTrack a univariate mean. Uses a compensation term for the update.\n\n#Note\n\nThis should be more accurate as Mean in most cases but the guarantees of KahanSum do not apply. merge! can have some accuracy issues.\n\nUpdate\n\nμ = (1 - γ) * μ + γ * x\n\nExample\n\n@time fit!(KahanMean(), randn(10^6))\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.KahanSum","page":"API","title":"OnlineStats.KahanSum","text":"KahanSum(T::Type = Float64)\n\nTrack the overall sum. Includes a compensation term that effectively doubles precision, see Wikipedia for details.\n\nExample\n\nfit!(KahanSum(Float64), fill(1, 100))\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.KahanVariance","page":"API","title":"OnlineStats.KahanVariance","text":"KahanVariance(; T=Float64, weight=EqualWeight())\n\nTrack the univariate variance. Uses compensation terms for a higher accuracy.\n\n#Note\n\nThis should be more accurate as Variance in most cases but the guarantees of KahanSum do not apply. merge! can have accuracy issues.\n\nExample\n\no = fit!(KahanVariance(), randn(10^6))\nmean(o)\nvar(o)\nstd(o)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.Lag","page":"API","title":"OnlineStats.Lag","text":"Lag(T, b::Int)\n\nStore the last b values for a data stream of type T.  Values are stored as\n\nv(t) v(t-1) v(t-2)  v(t-b+1)\n\nso that value(o::Lag)[1] is the most recent observation and value(o::Lag)[end] is the b-th most recent observation.\n\nExample\n\no = fit!(Lag(Int, 10), 1:12)\no[1]\no[end]\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.LinReg","page":"API","title":"OnlineStats.LinReg","text":"LinReg()\n\nLinear regression, optionally with element-wise ridge regularization.\n\nExample\n\nx = randn(100, 5)\ny = x * (1:5) + randn(100)\no = fit!(LinReg(), zip(eachrow(x),y))\ncoef(o)\ncoef(o, .1)\ncoef(o, [0,0,0,0,Inf])\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.LinRegBuilder","page":"API","title":"OnlineStats.LinRegBuilder","text":"LinRegBuilder(p)\n\nCreate an object from which any variable can be regressed on any other set of variables, optionally with element-wise ridge regularization.  The main function to use with LinRegBuilder is coef:\n\ncoef(o::LinRegBuilder, λ = 0; y=1, x=[2,3,...], bias=true, verbose=false)\n\nReturn the coefficients of a regressing column y on columns x with ridge (abs2 penalty) parameter λ.  An intercept (bias) term is added by default.\n\nExamples\n\nx = randn(1000, 10)\no = fit!(LinRegBuilder(), eachrow(x))\n\ncoef(o; y=3, verbose=true)\n\ncoef(o; y=7, x=[2,5,4])\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.LogSumExp","page":"API","title":"OnlineStats.LogSumExp","text":"LogSumExp(T::Type = Float64)\n\nFor positive numbers that can be very small (or very large), it's common to store each log(x) instead of each x itself, to avoid underflow (or overflow). LogSumExp takes values in this representation and adds them, returning the result in the same representation.\n\nRef: https://www.nowozin.net/sebastian/blog/streaming-log-sum-exp-computation.html\n\nExample\n\nx = randn(1000)\n\nfit!(LogSumExp(), x)\n\nlog(sum(exp.(x))) # should be very close\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.MSPI","page":"API","title":"OnlineStats.MSPI","text":"MSPI()\n\nMajorized Stochastic Proximal Iteration.\n\nReference: https://repository.lib.ncsu.edu/handle/1840.20/34945\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.Mosaic","page":"API","title":"OnlineStats.Mosaic","text":"Mosaic(T::Type, S::Type)\n\nData structure for generating a mosaic plot, a comparison between two categorical variables.\n\nExample\n\nusing OnlineStats, Plots\nx = [rand() > .8 for i in 1:10^5]\ny = rand([1,2,2,3,3,3], 10^5)\no = fit!(Mosaic(Bool, Int), zip(x, y))\nplot(o)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.MovingTimeWindow","page":"API","title":"OnlineStats.MovingTimeWindow","text":"MovingTimeWindow{T<:TimeType, S}(window::Dates.Period)\nMovingTimeWindow(window::Dates.Period; valtype=Float64, timetype=Date)\n\nFit a moving window of data based on time stamps.  Each observation must be a Tuple, NamedTuple, or Pair where the first item is <: Dates.TimeType.  Observations with a timestamp NOT in the range\n\nnow() - window ≤ timestamp ≤ now()\n\nare discarded on every fit!.\n\nExample\n\nusing Dates\ndts = Date(2010):Day(1):Date(2011)\ny = rand(length(dts))\n\no = MovingTimeWindow(Day(4); timetype=Date, valtype=Float64)\nfit!(o, zip(dts, y))\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.MovingWindow","page":"API","title":"OnlineStats.MovingWindow","text":"MovingWindow(b, T)\nMovingWindow(T, b)\n\nTrack a moving window of b items of type T.  Also known as a circular buffer.\n\nExample\n\no = MovingWindow(10, Int)\nfit!(o, 1:14)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.NBClassifier","page":"API","title":"OnlineStats.NBClassifier","text":"NBClassifier(p::Int, T::Type; stat = Hist(15))\n\nCalculate a naive bayes classifier for classes of type T and p predictors.  For each class K, predictor variables are summarized by the stat.\n\nExample\n\nx, y = randn(10^4, 10), rand(Bool, 10^4)\n\no = fit!(NBClassifier(10, Bool), zip(eachrow(x),y))\ncollect(keys(o))\nprobs(o)\n\nxi = randn(10)\npredict(o, xi)\nclassify(o, xi)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.OMAP","page":"API","title":"OnlineStats.OMAP","text":"OMAP()\n\nOnline MM via Averaged Parameter.\n\nReference: https://repository.lib.ncsu.edu/handle/1840.20/34945\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.OMAS","page":"API","title":"OnlineStats.OMAS","text":"OMAS()\n\nOnline MM via Averaged Surrogate.\n\nReference: https://repository.lib.ncsu.edu/handle/1840.20/34945\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.OrderStats","page":"API","title":"OnlineStats.OrderStats","text":"OrderStats(b::Int, T::Type = Float64; weight=EqualWeight())\n\nAverage order statistics with batches of size b.\n\nExample\n\no = fit!(OrderStats(100), randn(10^5))\nquantile(o, [.25, .5, .75])\n\nf = ecdf(o)\nf(0)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.P2Quantile","page":"API","title":"OnlineStats.P2Quantile","text":"P2Quantile(τ = 0.5)\n\nCalculate the approximate quantile via the P^2 algorithm.  It is more computationally expensive than the algorithms used by Quantile, but also more exact.\n\nRef: https://www.cse.wustl.edu/~jain/papers/ftp/psqr.pdf\n\nExample\n\nfit!(P2Quantile(.5), rand(10^5))\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.Partition","page":"API","title":"OnlineStats.Partition","text":"Partition(stat)\nPartition(stat, nparts)\n\nSplit a data stream into nparts (default 100) where each part is summarized by stat.\n\nExample\n\no = Partition(Extrema())\nfit!(o, cumsum(randn(10^5)))\n\nusing Plots\nplot(o)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.ProbMap","page":"API","title":"OnlineStats.ProbMap","text":"ProbMap(T::Type; weight=EqualWeight())\nProbMap(A::AbstractDict{T, Float64}; weight=EqualWeight())\n\nTrack a dictionary that maps unique values to its probability.  Similar to CountMap, but uses a weighting mechanism.\n\nExample\n\no = ProbMap(Int)\nfit!(o, rand(1:10, 1000))\nprobs(o)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.Quantile","page":"API","title":"OnlineStats.Quantile","text":"Quantile(q::Vector{Float64} = [0, .25, .5, .75, 1]; b=500)\n\nCalculate (approximate) quantiles from a data stream.  Internally uses ExpandingHist to estimate the distribution, for which b is the number of histogram bins.  Setting b to a larger number will increase accuracy at the cost of speed.\n\nExample\n\nq = [.25, .5, .75]\nx = randn(10^6)\n\no = fit!(Quantile(q, b=1000), randn(10^6))\nvalue(o)\n\nquantile(x, q)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.RMSPROP","page":"API","title":"OnlineStats.RMSPROP","text":"RMSPROP(α = .9)\n\nA Variation of ADAGRAD that uses element-wise weights generated by an exponentially weighted mean of the squared gradients.\n\nReference: https://ruder.io/optimizing-gradient-descent/\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.ReservoirSample","page":"API","title":"OnlineStats.ReservoirSample","text":"ReservoirSample(k::Int, T::Type = Float64)\n\nCreate a sample without replacement of size k.  After running through n observations, the probability of an observation being in the sample is 1 / n.\n\nExample\n\nfit!(ReservoirSample(100, Int), 1:1000)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.SGD","page":"API","title":"OnlineStats.SGD","text":"SGD()\n\nStochastic Gradient Descent.\n\nReference: https://ruder.io/optimizing-gradient-descent/\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.StatLag","page":"API","title":"OnlineStats.StatLag","text":"StatLag(stat, b)\n\nTrack a moving window (previous b copies) of stat.\n\nExample\n\nfit!(StatLag(Mean(), 10), 1:20)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.StatLearn","page":"API","title":"OnlineStats.StatLearn","text":"StatLearn(args...; penalty=zero, rate=LearningRate())\n\nFit a model (via stochastic approximation) that is linear in the parameters.  The (offline) objective function that StatLearn approximately minimizes is\n\n(1n) ᵢ f(yᵢ xᵢβ) + ⱼ λⱼ g(βⱼ)\n\nwhere fᵢ are loss functions of a response variable and linear predictor, λⱼs are nonnegative regularization parameters, and g is a penalty function.\n\nUse StatLearn with caution, as stochastic approximation algorithms are inherently noisy.\n\nArguments\n\nloss = OnlineStats.l2regloss: The loss function to be (approximately) minimized.\nRegression Losses:\nl2regloss: Squared error loss\nl1regloss: Absolute error loss\nClassification (y ∈ {-1, 1}) Losses:\nlogisticloss: Logistic regression\nl1hingeloss: Loss function used in Support Vector Machines.\nDWDLoss(q): Generalized Distance Weighted Discrimination (smoothed l1hingeloss)\nalgorithm = MSPI(): The stochastic approximation method to be used.\nAlgorithms based on Stochastic gradient:\nSGD(): Stochastic Gradient Descent\nADAGRAD(): AdaGrad (adaptive version of SGD)\nRMSPROP(): RMSProp (adaptive version of SGD)\nAlgorithms based on Majorization-Minimization Principle:\nMSPI(): Majorized Stochastic Proximal Iteration\nOMAS(): Online MM via Averaged Surrogate\nOMAP(): Online MM via Averaged Parameter\nλ = 0.0: The hyperparameter(s) used for the penalty function\nUser can provide elementwise penalty hyperparameters (Vector{Float64}) or single hyperparameter (Float64).\n\nKeyword Arguments\n\npenalty: The regularization function used.\nzero: no penalty (default)\nabs: (LASSO) parameters penalized by their absolute value\nabs2: (Ridge) parameters penalized by their squared value\nElasticNet(α): α * (abs penalty) + (1-α) * (abs2 penalty)\nrate = LearningRate(.6)\n\nExample\n\nx = randn(1000, 5)\ny = x * range(-1, stop=1, length=5) + randn(1000)\n\no = fit!(StatLearn(MSPI()), zip(eachrow(x), y))\ncoef(o)\n\no = fit!(StatLearn(OnlineStats.l1regloss, ADAGRAD()), zip(eachrow(x), y))\ncoef(o)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStats.Trace","page":"API","title":"OnlineStats.Trace","text":"Trace(stat, b=500, f=value)\n\nWrapper around an OnlineStat that stores b \"snapshots\" (a fixed copy of the OnlineStat).  The snapshots are taken at approximately equally-spaced intervals between 0 and the current nobs.  The main use case is visualizing state changes as observations are added.\n\nExample\n\nusing OnlineStats, Plots\n\no = Trace(Mean(), 10)\n\nfit!(o, 1:100)\n\nOnlineStats.snapshots(o)\n\nplot(o)\n\n\n\n\n\n","category":"type"},{"location":"api/#Base.sort!-Tuple{CCIPCA}","page":"API","title":"Base.sort!","text":"sort!(o::CCIPCA)\n\nSort eigenvalues and their eigenvectors of o so highest ones come first. Useful before visualising since it ensures most variation is on the first (X) axis.\n\n\n\n\n\n","category":"method"},{"location":"api/#OnlineStats.eigenvalue-Tuple{CCIPCA, Int64}","page":"API","title":"OnlineStats.eigenvalue","text":"eigenvalue(o::CCIPCA, i::Int)\n\nGet the ith eigenvalue of o. Also called principal variance for PCA.\n\n\n\n\n\n","category":"method"},{"location":"api/#OnlineStats.eigenvector-Tuple{CCIPCA, Int64}","page":"API","title":"OnlineStats.eigenvector","text":"eigenvector(o::CCIPCA, i::Int)\n\nGet the ith eigenvector of o.\n\n\n\n\n\n","category":"method"},{"location":"api/#OnlineStats.fittransform!-Tuple{CCIPCA, Vector{Float64}}","page":"API","title":"OnlineStats.fittransform!","text":"fittransform!(o::CCIPCA, u::Vector{Float64})\n\nFirst fit! and then transform the vector u into the PCA space represented by o.\n\n\n\n\n\n","category":"method"},{"location":"api/#OnlineStats.l1hingeloss-Tuple{Any, Any}","page":"API","title":"OnlineStats.l1hingeloss","text":"l1hingeloss(y, xβ)\n\nLoss function between boolean response y ∈ {-1, 1} and linear predictor xβ for support vector machines:\n\nmax(1 - y * xβ 0)\n\n\n\n\n\n","category":"method"},{"location":"api/#OnlineStats.l1regloss-Tuple{Any, Any}","page":"API","title":"OnlineStats.l1regloss","text":"l1regloss(y, xβ)\n\nLoss function between continuous response y and linear predictor xβ based on the L_1 norm:\n\ny - xβ\n\n\n\n\n\n","category":"method"},{"location":"api/#OnlineStats.l2regloss-Tuple{Any, Any}","page":"API","title":"OnlineStats.l2regloss","text":"l2regloss(y, xβ)\n\nLoss function between continuous response y and linear predictor xβ based on the L_2 norm:\n\n5 * (y - xβ) ^ 2\n\n\n\n\n\n","category":"method"},{"location":"api/#OnlineStats.logisticloss-Tuple{Any, Any}","page":"API","title":"OnlineStats.logisticloss","text":"logisticloss(y, xβ)\n\nLoss function between boolean response y ∈ {-1, 1} and linear predictor xβ for logistic regression:\n\nlog(1 + exp(-y * xβ))\n\n\n\n\n\n","category":"method"},{"location":"api/#OnlineStats.principalvar-Tuple{CCIPCA, Int64}","page":"API","title":"OnlineStats.principalvar","text":"eigenvalue(o::CCIPCA, i::Int)\n\nGet the ith eigenvalue of o. Also called principal variance for PCA.\n\n\n\n\n\n","category":"method"},{"location":"api/#OnlineStats.reconstruct-Tuple{CCIPCA, AbstractArray{Float64}}","page":"API","title":"OnlineStats.reconstruct","text":"reconstruct(o::CCIPCA, uproj::AbstractArray{Float64})\n\nReconstruct the (projected) vector uproj back to the original space from which o has been fitted.\n\n\n\n\n\n","category":"method"},{"location":"api/#OnlineStats.relativevariances-Tuple{CCIPCA}","page":"API","title":"OnlineStats.relativevariances","text":"relativevariances(o::CCIPCA)\n\nGet the relative variance (explained) in the direction of each eigenvector. Returns a vector of zeros if no vectors have yet been fitted. Note that this does not inclue the residual variance that is not captured in the eigenvectors.\n\n\n\n\n\n","category":"method"},{"location":"api/#OnlineStatsBase.value-Union{Tuple{DPMM{T}}, Tuple{T}} where T<:Real","page":"API","title":"OnlineStatsBase.value","text":"value(o::DPMM)\n\nRealize the mixture model where each component is the marginal predictive distribution obtained as\n\nq(x; mₖ, lₖ, aₖ, bₖ)\n    = ∫ N(x; μₖ, sqrt(1/τₖ)) q(μₖ | τₖ; mₖ, lₖ) q(τₖ; aₖ, bₖ) dμₖ dτₖ\n    = ∫ N(x; mₖ, sqrt(1/τₖ + 1/(lₖ*τₖ))) Gamma(τₖ; aₖ, bₖ) dμₖ dτₖ\n    = TDist( 2*aₖ, mₖ, sqrt(bₖ/aₖ*(lₖ+1)/lₖ) )\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsAPI.confint","page":"API","title":"StatsAPI.confint","text":"confint(b::Bootstrap, coverageprob = .95)\n\nReturn a confidence interval for a Bootstrap b.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.transform-Tuple{CCIPCA, AbstractArray{Float64}}","page":"API","title":"StatsBase.transform","text":"transform(o::CCIPCA, u::AbstractArray{Float64})\n\nTransform (i.e. project) the vector u into the PCA space represented by o.\n\n\n\n\n\n","category":"method"},{"location":"api/#OnlineStatsBase.CircBuff","page":"API","title":"OnlineStatsBase.CircBuff","text":"CircBuff(T, b; rev=false)\n\nCreate a fixed-length circular buffer of b items of type T.\n\nrev=false: o[1] is the oldest.\nrev=true: o[end] is the oldest.\n\nExample\n\na = CircBuff(Int, 5)\nb = CircBuff(Int, 5, rev=true)\n\nfit!(a, 1:10)\nfit!(b, 1:10)\n\na[1] == b[end] == 1\na[end] == b[1] == 10\n\nvalue(o; ordered=false)  # Retrieve values (no copy) without ordering\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStatsBase.CountMap","page":"API","title":"OnlineStatsBase.CountMap","text":"CountMap(T::Type)\nCountMap(dict::AbstractDict{T, Int})\n\nTrack a dictionary that maps unique values to its number of occurrences.  Similar to StatsBase.countmap.\n\nCounts can be incremented by values other than one (and decremented) using the fit!(::CountMap{T}, ::Pair{T,Int}) method, e.g.\n\no = fit!(CountMap(String), [\"A\", \"B\"])\nfit!(o, \"A\" => 5)\nfit!(o, \"A\" => -1)\n\nExample\n\no = fit!(CountMap(Int), rand(1:10, 1000))\nvalue(o)\nOnlineStatsBase.probs(o)\nOnlineStats.pdf(o, 1)\ncollect(keys(o))\nsort!(o)\ndelete!(o, 1)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStatsBase.CountMissing","page":"API","title":"OnlineStatsBase.CountMissing","text":"CountMissing(stat)\n\nCalculate a stat along with the count of missing values.  \n\nExample\n\no = CountMissing(Mean())\nfit!(o, [1, missing, 3])\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStatsBase.Counter","page":"API","title":"OnlineStatsBase.Counter","text":"Counter(T=Number)\n\nCount the number of items in a data stream with elements of type T.\n\nExample\n\nfit!(Counter(Int), 1:100)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStatsBase.CovMatrix","page":"API","title":"OnlineStatsBase.CovMatrix","text":"CovMatrix(p=0; weight=EqualWeight())\nCovMatrix(::Type{T}, p=0; weight=EqualWeight())\n\nCalculate a covariance/correlation matrix of p variables.  If the number of variables is unknown, leave the default p=0.\n\nExample\n\no = fit!(CovMatrix(), randn(100, 4) |> eachrow)\ncor(o)\ncov(o)\nmean(o)\nvar(o)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStatsBase.Extrema","page":"API","title":"OnlineStatsBase.Extrema","text":"Extrema(T::Type = Float64)\nExtrema(min_init::T, max_init::T)\n\nMaximum and minimum (and number of occurrences for each) for a data stream of type T.\n\nExample\n\nExtrema(Float64) == Extrema(Inf, -Inf)\n\no = fit!(Extrema(), rand(10^5))\nextrema(o)\nmaximum(o)\nminimum(o)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStatsBase.ExtremeValues","page":"API","title":"OnlineStatsBase.ExtremeValues","text":"ExtremeValues(T = Float64, b=25)\n\nTrack the b smallest and largest values of a data stream (as well as how many times that value occurred).\n\nExample\n\no = ExtremeValues(Int, 3)\n\nfit!(o, 1:100)\n\nvalue(o).lo  # [1 => 1, 2 => 1, 3 => 1]\n\nvalue(o).hi  # [98 => 1, 99 => 1, 100 => 1]\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStatsBase.FilterTransform","page":"API","title":"OnlineStatsBase.FilterTransform","text":"FilterTransform(stat::OnlineStat{S}, T = S; filter = x->true, transform = identity)\nFilterTransform(T => filter => transform => stat)\n\nWrapper around an OnlineStat that the filters and transforms its input.  Note that, depending on  your transformation, you may need to specify the type of a single observation (T).\n\nExamples\n\no = FilterTransform(Mean(), Union{Missing,Number}, filter=!ismissing)\nfit!(o, [1, missing, 3])\n\no = FilterTransform(String => (x->true) => (x->parse(Int,x)) => Mean())\nfit!(o, \"1\")\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStatsBase.Group","page":"API","title":"OnlineStatsBase.Group","text":"Group(stats::OnlineStat...)\nGroup(; stats...)\nGroup(collection)\n\nCreate a vector-input stat from several scalar-input stats.  For a new observation y, y[i] is sent to stats[i].\n\nExamples\n\nx = randn(100, 2)\n\nfit!(Group(Mean(), Mean()), eachrow(x))\nfit!(Group(Mean(), Variance()), eachrow(x))\n\no = fit!(Group(m1 = Mean(), m2 = Mean()), eachrow(x))\no.stats.m1\no.stats.m2\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStatsBase.GroupBy","page":"API","title":"OnlineStatsBase.GroupBy","text":"GroupBy(T, stat)\n\nUpdate stat for each group (of type T).  A single observation is either a (named) tuple with two elements or a Pair.\n\nExample\n\nx = rand(Bool, 10^5)\ny = x .+ randn(10^5)\nfit!(GroupBy(Bool, Series(Mean(), Extrema())), zip(x,y))\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStatsBase.Mean","page":"API","title":"OnlineStatsBase.Mean","text":"Mean(T = Float64; weight=EqualWeight())\n\nTrack a univariate mean, stored as type T.\n\nExample\n\n@time fit!(Mean(), randn(10^6))\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStatsBase.Moments","page":"API","title":"OnlineStatsBase.Moments","text":"Moments(; weight=EqualWeight())\n\nFirst four non-central moments.\n\nExample\n\no = fit!(Moments(), randn(1000))\nmean(o)\nvar(o)\nstd(o)\nskewness(o)\nkurtosis(o)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStatsBase.RepeatingRange","page":"API","title":"OnlineStatsBase.RepeatingRange","text":"RepeatingRange(rng)\n\nRange that repeats forever. e.g.\n\nr = OnlineStatsBase.RepeatingRange(1:2)\nr[1:5] == [1, 2, 1, 2, 1]\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStatsBase.Series","page":"API","title":"OnlineStatsBase.Series","text":"Series(stats)\nSeries(stats...)\nSeries(; stats...)\n\nTrack a collection stats for one data stream.\n\nExample\n\ns = Series(Mean(), Variance())\nfit!(s, randn(1000))\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStatsBase.SkipMissing","page":"API","title":"OnlineStatsBase.SkipMissing","text":"SkipMissing(stat)\n\nWrapper around an OnlineStat that will skip over missing values.\n\nExample\n\no = SkipMissing(Mean())\n\nfit!(o, [1, missing, 3])\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStatsBase.Sum","page":"API","title":"OnlineStatsBase.Sum","text":"Sum(T::Type = Float64)\n\nTrack the overall sum.\n\nExample\n\nfit!(Sum(Int), fill(1, 100))\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStatsBase.TryCatch","page":"API","title":"OnlineStatsBase.TryCatch","text":"TryCatch(stat; error_limit=1000, error_message_limit=90)\n\nWrap each call to fit! in a try-catch block and track the errors encountered (via CountMap).   Only error_limit unique errors will be included in the CountMap.  If a new error occurs after  error_limit has been reached, it will be included in the CountMap as \"Other\".  Only the first  error_message_limit characters of each error message will be recorded.\n\nExample\n\no = TryCatch(Mean())\n\nfit!(o, [1, missing, 3])\n\nOnlineStatsBase.errors(o)\n\n\n\n\n\n","category":"type"},{"location":"api/#OnlineStatsBase.Variance","page":"API","title":"OnlineStatsBase.Variance","text":"Variance(T = Float64; weight=EqualWeight())\n\nUnivariate variance, tracked as type T.\n\nExample\n\no = fit!(Variance(), randn(10^6))\nmean(o)\nvar(o)\nstd(o)\n\n\n\n\n\n","category":"type"},{"location":"api/#Base.merge!-Tuple{OnlineStat, OnlineStat}","page":"API","title":"Base.merge!","text":"merge!(a, b)\n\nMerge OnlineStat b into a (supported by most OnlineStat types).\n\nExample\n\na = fit!(Mean(), 1:10)\nb = fit!(Mean(), 11:20)\nmerge!(a, b)\n\n\n\n\n\n","category":"method"},{"location":"api/#OnlineStatsBase.smooth!-Tuple{Any, Any, Any}","page":"API","title":"OnlineStatsBase.smooth!","text":"smooth!(a, b, γ)\n\nUpdate a in place by applying the smooth function elementwise with b.\n\n\n\n\n\n","category":"method"},{"location":"api/#OnlineStatsBase.smooth-Tuple{Any, Any, Any}","page":"API","title":"OnlineStatsBase.smooth","text":"smooth(a, b, γ)\n\nWeighted average of a and b with weight γ.\n\n(1 - γ) * a + γ * b\n\n\n\n\n\n","category":"method"},{"location":"api/#OnlineStatsBase.smooth_syr!-Tuple{AbstractMatrix, Any, Any}","page":"API","title":"OnlineStatsBase.smooth_syr!","text":"smooth_syr!(A::AbstractMatrix, x, γ::Number)\n\nWeighted average of symmetric rank-1 update.  Updates the upper triangle of:\n\nA = (1 - γ) * A + γ * x * x'\n\n\n\n\n\n","category":"method"},{"location":"api/#OnlineStatsBase.value-Tuple{T} where T<:OnlineStat","page":"API","title":"OnlineStatsBase.value","text":"value(stat::OnlineStat)\n\nCalculate the value of stat from its \"sufficient statistics\".\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsAPI.fit!-Tuple{OnlineStat, OnlineStat}","page":"API","title":"StatsAPI.fit!","text":"fit!(stat1::OnlineStat, stat2::OnlineStat)\n\nAlias for merge!. Merges stat2 into stat1.\n\nUseful for reductions of OnlineStats using fit!.\n\nExample\n\njulia> v = [reduce(fit!, [1, 2, 3], init=Mean()) for _ in 1:3]\n3-element Vector{Mean{Float64, EqualWeight}}:\nMean: n=3 | value=2.0\nMean: n=3 | value=2.0\nMean: n=3 | value=2.0\n\njulia> reduce(fit!, v, init=Mean())\nMean: n=9 | value=2.0\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsAPI.fit!-Union{Tuple{T}, Tuple{OnlineStat{T}, T, Integer}} where T","page":"API","title":"StatsAPI.fit!","text":"fit!(stat::OnlineStat, y, n)\n\nUpdate the \"sufficient statistics\" of a stat with multiple observations of a single value. Unless a specialized formula is used, fit!(Mean(), 10, 5) is equivalent to:\n\no = Mean()\n\nfor _ in 1:5\n    fit!(o, 10)\nend\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsAPI.fit!-Union{Tuple{T}, Tuple{OnlineStat{T}, T}} where T","page":"API","title":"StatsAPI.fit!","text":"fit!(stat::OnlineStat, data)\n\nUpdate the \"sufficient statistics\" of a stat with more data.   If typeof(data) is not the type of a single observation for the provided stat, fit! will attempt to iterate through and fit! each item in data.  Therefore, fit!(Mean(), 1:10) translates roughly to:\n\no = Mean()\n\nfor x in 1:10\n    fit!(o, x)\nend\n\n\n\n\n\n","category":"method"},{"location":"weights/#Weights","page":"Weights","title":"Weights","text":"","category":"section"},{"location":"weights/","page":"Weights","title":"Weights","text":"Many OnlineStats are parameterized by a Weight that controls the influence of new observations.  If the OnlineStat is capable of calculating the same result as a corresponding offline estimator, it will have a keyword argument weight.  If the OnlineStat uses stochastic approximation, it will have a keyword argument rate (see this great resource on stochastic approximation algorithms).","category":"page"},{"location":"weights/","page":"Weights","title":"Weights","text":"Consider how weights affect the influence of the next observation on an online mean theta^(t), as many OnlineStats use updates of this form.  A larger weight  gamma_t puts higher influence on the new observation x_t:","category":"page"},{"location":"weights/","page":"Weights","title":"Weights","text":"theta^(t) = (1-gamma_t)theta^(t-1) + gamma_t x_t","category":"page"},{"location":"weights/","page":"Weights","title":"Weights","text":"note: Note\nThe values produced by a Weight must follow two rules:gamma_1 = 1 (guarantees theta^(1) = x_1)\ngamma_t in (0 1) quad forall t  1 (guarantees theta^(t) stays inside a convex space)","category":"page"},{"location":"weights/","page":"Weights","title":"Weights","text":"using Plots, OnlineStats, OnlineStatsBase, InteractiveUtils # hide\ngr(size=(816,400), margin=6Plots.mm) # hide\nws = subtypes(OnlineStatsBase.Weight) # hide\np = plot(ws[1](), st=:line, c=1, primary=[true false], lw=3, title=\"Built-in Weights\") # hide\nfor i in 2:length(ws) # hide\n    plot!(p, ws[i](), st=:line, c=i, primary=[true false], lw=3, linestyle=:auto) # hide\nend # hide\np # hide","category":"page"},{"location":"weights/","page":"Weights","title":"Weights","text":"info: Info\nThe notion of weighting in OnlineStats is fundamentally different than StatsBase.AbstractWeights.","category":"page"},{"location":"weights/","page":"Weights","title":"Weights","text":"In OnlineStats, a weight determines the influence of an observation compared to the current state of the statistic.\nIn StatsBase, a weight determines the influence of an observation in the overall calculation of the statistic.","category":"page"},{"location":"weights/","page":"Weights","title":"Weights","text":"using OnlineStats, StatsBase\n\nx = 1:99;\nw = fill(0.1, 99);\n\n# StatsBase: All weights == 0.1\nmean(x) ≈ mean(x, aweights(w)) ≈ mean(x, fweights(w)) ≈ mean(x, pweights(w))\n\n# OnlineStats: All weights == 0.1\no = fit!(Mean(weight = n -> 0.1), x)\n\n\nmean(x)  # Every observation has equal influence over statistic.\nvalue(o)  # Recent observations have higher influence over statistic.","category":"page"},{"location":"weights/#Weight-Types","page":"Weights","title":"Weight Types","text":"","category":"section"},{"location":"weights/","page":"Weights","title":"Weights","text":"EqualWeight\nExponentialWeight\nLearningRate\nLearningRate2\nHarmonicWeight\nMcclainWeight","category":"page"},{"location":"weights/#OnlineStatsBase.EqualWeight","page":"Weights","title":"OnlineStatsBase.EqualWeight","text":"EqualWeight()\n\nEqually weighted observations.\n\nγ(t) = 1  t\n\n\n\n\n\n","category":"type"},{"location":"weights/#OnlineStatsBase.ExponentialWeight","page":"Weights","title":"OnlineStatsBase.ExponentialWeight","text":"ExponentialWeight(λ::Float64)\nExponentialWeight(lookback::Int)\n\nExponentially weighted observations.  Each weight is λ = 2 / (lookback + 1).  \n\nExponentialWeight does not satisfy the usual assumption that γ(1) == 1.  Therefore, some statistics have an implicit starting value. \n\n# E.g. Mean has an implicit starting value of 0.\no = Mean(weight=ExponentialWeight(.1))\nfit!(o, 10)\nvalue(o) == 1\n\nγ(t) = λ\n\n\n\n\n\n","category":"type"},{"location":"weights/#OnlineStatsBase.LearningRate","page":"Weights","title":"OnlineStatsBase.LearningRate","text":"LearningRate(r = .6)\n\nSlowly decreasing weight.  Satisfies the standard stochastic approximation assumption  γ(t) =   γ(t)^2   if r  (5 1.\n\nγ(t) = inv(t ^ r)\n\n\n\n\n\n","category":"type"},{"location":"weights/#OnlineStatsBase.LearningRate2","page":"Weights","title":"OnlineStatsBase.LearningRate2","text":"LearningRate2(c = .5)\n\nSlowly decreasing weight.\n\nγ(t) = inv(1 + c * (t - 1))\n\n\n\n\n\n","category":"type"},{"location":"weights/#OnlineStatsBase.HarmonicWeight","page":"Weights","title":"OnlineStatsBase.HarmonicWeight","text":"HarmonicWeight(a = 10.0)\n\nWeight determined by harmonic series.\n\nγ(t) = a  (a + t - 1)\n\n\n\n\n\n","category":"type"},{"location":"weights/#OnlineStatsBase.McclainWeight","page":"Weights","title":"OnlineStatsBase.McclainWeight","text":"McclainWeight(α = .1)\n\nWeight which decreases into a constant.\n\nγ(t) = γ(t-1)  (1 + γ(t-1) - α)\n\n\n\n\n\n","category":"type"},{"location":"weights/#Custom-Weighting","page":"Weights","title":"Custom Weighting","text":"","category":"section"},{"location":"weights/","page":"Weights","title":"Weights","text":"The Weight can be any callable object that receives the number of observations as its argument.  For example:","category":"page"},{"location":"weights/","page":"Weights","title":"Weights","text":"weight = inv will have the same result as weight = EqualWeight().\nweight = x -> .01 will have the same result as weight = ExponentialWeight(.01)","category":"page"},{"location":"weights/","page":"Weights","title":"Weights","text":"using OnlineStats # hide\ny = randn(100);\n\nfit!(Mean(weight = EqualWeight()), y)\nfit!(Mean(weight = inv), y)","category":"page"},{"location":"weights/#Example-of-Weight-Effects-using-Data-with-[Concept-Drift](https://en.wikipedia.org/wiki/Concept_drift)","page":"Weights","title":"Example of Weight Effects using Data with Concept Drift","text":"","category":"section"},{"location":"weights/","page":"Weights","title":"Weights","text":"<br>\n<img src=\"https://user-images.githubusercontent.com/8075494/57347308-d4d27d00-711f-11e9-8fbe-fc4523b96b48.gif\" style=\"width:400\">","category":"page"},{"location":"howfitworks/","page":"Details of Updating (fit!)","title":"Details of Updating (fit!)","text":"using OnlineStats","category":"page"},{"location":"howfitworks/#Details-of-Updating-(fit!)","page":"Details of Updating (fit!)","title":"Details of Updating (fit!)","text":"","category":"section"},{"location":"howfitworks/#Core-Principles","page":"Details of Updating (fit!)","title":"Core Principles","text":"","category":"section"},{"location":"howfitworks/","page":"Details of Updating (fit!)","title":"Details of Updating (fit!)","text":"Stats are subtypes of OnlineStat{T} where T is the type of a single observation.\nE.g. Mean <: OnlineStat{Number}\nfit!(o::OnlineStat{T}, data::T)\nUpdate o with the single observation data.\nfit!(o::OnlineStat{T}, data::S)\nIterate through data and fit! each item.","category":"page"},{"location":"howfitworks/#Why-is-Fitting-Based-on-Iteration?","page":"Details of Updating (fit!)","title":"Why is Fitting Based on Iteration?","text":"","category":"section"},{"location":"howfitworks/#Reason-1:-OnlineStats-doesn't-make-assumptions-on-the-shape-of-your-data","page":"Details of Updating (fit!)","title":"Reason 1: OnlineStats doesn't make assumptions on the shape of your data","text":"","category":"section"},{"location":"howfitworks/","page":"Details of Updating (fit!)","title":"Details of Updating (fit!)","text":"Consider CovMatrix, for which a single observation is an AbstractVector, Tuple, or NamedTuple. If I try to fit! it with a Matrix, it's ambiguous whether I want rows or columns of the matrix to be treated as individual observations.","category":"page"},{"location":"howfitworks/","page":"Details of Updating (fit!)","title":"Details of Updating (fit!)","text":"note: Note\nSee eachrow and eachcol (after typing using LinearAlgebra).","category":"page"},{"location":"howfitworks/","page":"Details of Updating (fit!)","title":"Details of Updating (fit!)","text":"x = randn(1000, 2)\n\nfit!(CovMatrix(), eachrow(x))\n\nfit!(CovMatrix(), eachcol(x'))","category":"page"},{"location":"howfitworks/#Reason-2:-OnlineStats-works-out-of-the-box-with-many-data-structures","page":"Details of Updating (fit!)","title":"Reason 2: OnlineStats works out-of-the-box with many data structures","text":"","category":"section"},{"location":"howfitworks/","page":"Details of Updating (fit!)","title":"Details of Updating (fit!)","text":"Tabular data structures such as those in JuliaDB iterate over named tuples of rows, so things like this just work:","category":"page"},{"location":"howfitworks/","page":"Details of Updating (fit!)","title":"Details of Updating (fit!)","text":"using JuliaDB\n\nt = table(randn(100), randn(100))\n\nfit!(2Mean(), t)","category":"page"},{"location":"howfitworks/#A-Common-Error","page":"Details of Updating (fit!)","title":"A Common Error","text":"","category":"section"},{"location":"howfitworks/","page":"Details of Updating (fit!)","title":"Details of Updating (fit!)","text":"Consider the following example:","category":"page"},{"location":"howfitworks/","page":"Details of Updating (fit!)","title":"Details of Updating (fit!)","text":"fit!(Mean(), \"asdf\")","category":"page"},{"location":"howfitworks/","page":"Details of Updating (fit!)","title":"Details of Updating (fit!)","text":"This causes an error because:","category":"page"},{"location":"howfitworks/","page":"Details of Updating (fit!)","title":"Details of Updating (fit!)","text":"\"asdf\" is not a Number, so OnlineStats attempts to iterate through it\nIterating through \"asdf\" begins with the character 'a'","category":"page"},{"location":"stats_and_models/","page":"Statistics and Models","title":"Statistics and Models","text":"ENV[\"GKSwstype\"] = \"100\"\nENV[\"GKS_ENCODING\"]=\"utf8\"","category":"page"},{"location":"stats_and_models/#Statistics-and-Models","page":"Statistics and Models","title":"Statistics and Models","text":"","category":"section"},{"location":"stats_and_models/#Univariate-Statistics","page":"Statistics and Models","title":"Univariate Statistics","text":"","category":"section"},{"location":"stats_and_models/","page":"Statistics and Models","title":"Statistics and Models","text":"Statistic OnlineStat\nMean Mean\nVariance Variance\nQuantiles Quantile, OrderStats, and P2Quantile\nMaximum/Minimum Extrema\nSkewness and kurtosis Moments\nSum Sum\nGeometric Mean GeometricMean","category":"page"},{"location":"stats_and_models/#Plotting-(See-[Data-Visualization](@ref))","page":"Statistics and Models","title":"Plotting (See Data Visualization)","text":"","category":"section"},{"location":"stats_and_models/","page":"Statistics and Models","title":"Statistics and Models","text":"info: Info\nMany OnlineStats have Plot recipes beyond what is listed here.","category":"page"},{"location":"stats_and_models/","page":"Statistics and Models","title":"Statistics and Models","text":"Plot OnlineStat\nBig Data Viz Partition, IndexedPartition, KIndexedPartition\nMosaic Plot Mosaic\nHeatMap HeatMap","category":"page"},{"location":"stats_and_models/#Time-Series","page":"Statistics and Models","title":"Time Series","text":"","category":"section"},{"location":"stats_and_models/","page":"Statistics and Models","title":"Statistics and Models","text":"Statistic OnlineStat\nDifference Diff\nLag Lag\nAutocorrelation/autocovariance AutoCov\nTracked history Trace, StatLag","category":"page"},{"location":"stats_and_models/#Multivariate-Analysis","page":"Statistics and Models","title":"Multivariate Analysis","text":"","category":"section"},{"location":"stats_and_models/","page":"Statistics and Models","title":"Statistics and Models","text":"Statistic/Model OnlineStat\nCovariance/correlation matrix CovMatrix\nPrincipal components analysis CovMatrix, CCIPCA\nK-means clustering KMeans\nMultiple univariate statistics Group","category":"page"},{"location":"stats_and_models/#Nonparametric-Density-Estimation","page":"Statistics and Models","title":"Nonparametric Density Estimation","text":"","category":"section"},{"location":"stats_and_models/","page":"Statistics and Models","title":"Statistics and Models","text":"Statistic/Model OnlineStat\nHistograms/continuous density Hist, KHist, and ExpandingHist\nASH density (semiparametric, similar to KDE) Ash\nApproximate order statistics OrderStats\nCount for each unique value CountMap\nApproximate CDF OrderStats","category":"page"},{"location":"stats_and_models/#Parametric-Density-Estimation","page":"Statistics and Models","title":"Parametric Density Estimation","text":"","category":"section"},{"location":"stats_and_models/","page":"Statistics and Models","title":"Statistics and Models","text":"Distribution OnlineStat\nBeta FitBeta\nCauchy FitCauchy\nGamma FitGamma\nLogNormal FitLogNormal\nNormal FitNormal\nMultinomial FitMultinomial\nMvNormal FitMvNormal","category":"page"},{"location":"stats_and_models/#Machine/Statistical-Learning","page":"Statistics and Models","title":"Machine/Statistical Learning","text":"","category":"section"},{"location":"stats_and_models/","page":"Statistics and Models","title":"Statistics and Models","text":"Model OnlineStat\nLinear (also ridge) regression LinReg, LinRegBuilder\nDecision Trees FastTree\nRandom Forest FastForest\nNaive Bayes Classifier NBClassifier\nML via Stochastic Approximation StatLearn","category":"page"},{"location":"stats_and_models/#Other","page":"Statistics and Models","title":"Other","text":"","category":"section"},{"location":"stats_and_models/","page":"Statistics and Models","title":"Statistics and Models","text":"Statistic/Model OnlineStat\nHandling Missing Data FilterTransform, CountMissing, SkipMissing\nStatistical Bootstrap Bootstrap\nApprox. count of distinct elements HyperLogLog\nApprox. count of occurrences CountMinSketch\nRandom sample ReservoirSample\nMoving Window MovingWindow, MovingTimeWindow","category":"page"},{"location":"stats_and_models/#Collection-of-Stats","page":"Statistics and Models","title":"Collection of Stats","text":"","category":"section"},{"location":"stats_and_models/","page":"Statistics and Models","title":"Statistics and Models","text":"Statistic/Model OnlineStat\nUnivariate data stream Series\nMultivariate data streams Group\nGroup by categorical variable GroupBy","category":"page"},{"location":"stats_and_models/#Stochastic-Approximation-with-[StatLearn](@ref)","page":"Statistics and Models","title":"Stochastic Approximation with StatLearn","text":"","category":"section"},{"location":"stats_and_models/#Regression-and-Classification-Losses","page":"Statistics and Models","title":"Regression and Classification Losses","text":"","category":"section"},{"location":"stats_and_models/","page":"Statistics and Models","title":"Statistics and Models","text":"Loss Function\nL_2 Loss (squared error) OnlineStats.l2regloss\nL_1 Loss (absolute error) OnlineStats.l1regloss\nLogistic Loss OnlineStats.logisticloss\nL_1 Hinge Loss OnlineStats.l1hingeloss\nGeneralized distance weighted discrimination OnlineStats.DWDLoss","category":"page"},{"location":"stats_and_models/#Penalty/regularization-functions","page":"Statistics and Models","title":"Penalty/regularization functions","text":"","category":"section"},{"location":"stats_and_models/","page":"Statistics and Models","title":"Statistics and Models","text":"Penalty Function\nNone zero\nLASSO (L_1 penalty) abs\nRidge  (L_2 penalty) abs2\nElastic Net OnlineStats.ElasticNet","category":"page"},{"location":"stats_and_models/#Optimization-Algorithms","page":"Statistics and Models","title":"Optimization Algorithms","text":"","category":"section"},{"location":"stats_and_models/","page":"Statistics and Models","title":"Statistics and Models","text":"Algorithm Constructor\nStochastic Gradient Descent SGD\nRMSProp RMSPROP\nAdaGrad ADAGRAD\nAdaDelta ADADELTA\nADAM ADAM\nADAMax ADAMAX\nMSPI (Majorized Stochastic Proximal Iteration) MSPI\nOnline Majorization-Minimization (MM) - averaged surrogate OMAS\nOnline MM - Averaged Parameter OMAP","category":"page"},{"location":"demos/#Demos","page":"Demos","title":"Demos","text":"","category":"section"},{"location":"demos/","page":"Demos","title":"Demos","text":"A collection of jupyter notebooks are hosted at https://github.com/joshday/OnlineStatsDemos.  ","category":"page"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"import Random\nusing Dates\nusing OnlineStats\nusing Plots\ngr(size=(816,400), margin=6Plots.mm)\nRandom.seed!(1234)","category":"page"},{"location":"dataviz/#Data-Visualization","page":"Data Visualization","title":"Data Visualization","text":"","category":"section"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"note: Note\nEach of the following examples plots one million data points, but can scale to infinitely many observations, since only a summary (OnlineStat) of the data is plotted.","category":"page"},{"location":"dataviz/#Partitions","page":"Data Visualization","title":"Partitions","text":"","category":"section"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"The Partition type summarizes sections of a data stream using any OnlineStat, and is therefore extremely useful in visualizing huge datasets, as summaries are plotted rather than every single observation.","category":"page"},{"location":"dataviz/#Continuous-Data","page":"Data Visualization","title":"Continuous Data","text":"","category":"section"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"y = cumsum(randn(10^6)) + 100randn(10^6)\n\no = Partition(KHist(10))\n\nfit!(o, y)\n\nplot(o)","category":"page"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"o = Partition(Series(Mean(), Extrema()))\n\nfit!(o, y)\n\nplot(o)","category":"page"},{"location":"dataviz/#Categorical-Data","page":"Data Visualization","title":"Categorical Data","text":"","category":"section"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"y = rand([\"a\", \"a\", \"b\", \"c\"], 10^6)\n\no = Partition(CountMap(String), 75)\n\nfit!(o, y)\n\nplot(o)","category":"page"},{"location":"dataviz/#Indexed-Partitions","page":"Data Visualization","title":"Indexed Partitions","text":"","category":"section"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"The Partition type can only track the number of observations in the x-axis.  If you wish to plot one variable against another, you can use an IndexedPartition.","category":"page"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"x = randn(10^6)\ny = x + randn(10^6)\n\no = fit!(IndexedPartition(Float64, KHist(40), 40), zip(x, y))\n\nplot(o)","category":"page"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"x = rand(10^6)\ny = rand(1:5, 10^6)\n\no = fit!(IndexedPartition(Float64, CountMap(Int)), zip(x,y))\n\nplot(o, xlab = \"X\", ylab = \"Y\")","category":"page"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"Due to a limitation with Plots.jl, Date and DateTime will sometimes be converted to their Dates.value when plotted.  To get human-readable tick labels, you can use the xformatter keyword argument to plot.","category":"page"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"using Dates\n\nx = rand(Date(2019):Day(1):Date(2020), 10^6)\ny = Dates.value.(x) .+ 30randn(10^6)\n\no = fit!(IndexedPartition(Date, KHist(20)), zip(x,y))\n\nplot(o, xformatter = x -> string(Date(Dates.UTInstant(Day(x)))))","category":"page"},{"location":"dataviz/#K-Indexed-Partitions","page":"Data Visualization","title":"K-Indexed Partitions","text":"","category":"section"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"A KIndexedPartition is simlar to an IndexedPartition, but uses a different method of binning the x variable (centroids vs. intervals), similar to that of KHist.","category":"page"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"For the sake of performance, you must provide a function that creates the OnlineStat you wish to calculate for the y variable.","category":"page"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"x = randn(10^6)\ny = x + randn(10^6)\n\no = fit!(KIndexedPartition(Float64, () -> KHist(20)), zip(x, y))\n\nplot(o)","category":"page"},{"location":"dataviz/#Histograms","page":"Data Visualization","title":"Histograms","text":"","category":"section"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"s = fit!(Series(KHist(25), Hist(-5:.2:5), ExpandingHist(100)), randn(10^6))\nplot(s, link = :x, label = [\"KHist\" \"Hist\" \"ExpandingHist\"])","category":"page"},{"location":"dataviz/#Average-Shifted-Histograms-(ASH)","page":"Data Visualization","title":"Average Shifted Histograms (ASH)","text":"","category":"section"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"ASH is a semi-parametric density estimation method that is similar to Kernel Density Estimation, but uses a fine partition histogram instead of individual observations to perform the smoothing.","category":"page"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"o = fit!(Ash(ExpandingHist(1000), 5), randn(10^6))\nplot(o)","category":"page"},{"location":"dataviz/#Approximate-CDF","page":"Data Visualization","title":"Approximate CDF","text":"","category":"section"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"o = fit!(OrderStats(1000), randn(10^6))\n\nplot(o)","category":"page"},{"location":"dataviz/#Mosaic-Plots","page":"Data Visualization","title":"Mosaic Plots","text":"","category":"section"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"The Mosaic type allows you to plot the relationship between two categorical variables. It is typically more useful than a bar plot, as class probabilities are given by the horizontal widths.","category":"page"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"using RDatasets\nt = dataset(\"ggplot2\", \"diamonds\")\n\no = Mosaic(eltype(t.Cut), eltype(t.Color))\n\nfit!(o, zip(t.Cut, t.Color))\n\nplot(o, legendtitle=\"Color\", xlabel=\"Cut\")","category":"page"},{"location":"dataviz/#HeatMap","page":"Data Visualization","title":"HeatMap","text":"","category":"section"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"o = HeatMap(-5:.1:5, -0:.1:10)\n\nx, y = randn(10^6), 5 .+ randn(10^6)\n\nfit!(o, zip(x, y))\n\nplot(o)","category":"page"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"plot(o, marginals=false, legend=true)","category":"page"},{"location":"dataviz/#Traces","page":"Data Visualization","title":"Traces","text":"","category":"section"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"A Trace will take snapshots of an OnlineStat as it is fitted, allowing you view how the value changed as observations were added.  This can be useful for identifying concept drift or finding optimal hyperparameters for stochastic approximation methods like StatLearn.","category":"page"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"y = range(1, 20, length=10^6) .* randn(10^6)\n\no = Trace(Extrema())\n\nfit!(o, y)\n\nplot(o)","category":"page"},{"location":"dataviz/#Naive-Bayes-Classifier","page":"Data Visualization","title":"Naive Bayes Classifier","text":"","category":"section"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"The NBClassifier type stores conditional histograms of the predictor variables, allowing you to plot approximate \"group by\" distributions:","category":"page"},{"location":"dataviz/","page":"Data Visualization","title":"Data Visualization","text":"# make data\nx = randn(10^6, 5)\ny = x * [1,3,5,7,9] .> 0\n\no = NBClassifier(5, Bool)  # 5 predictors with Boolean categories\nfit!(o, zip(eachrow(x), y))\nplot(o)","category":"page"},{"location":"bigdata/","page":"Big Data","title":"Big Data","text":"ENV[\"GKSwstype\"] = \"100\"\nENV[\"GKS_ENCODING\"]=\"utf8\"","category":"page"},{"location":"bigdata/#Big-Data","page":"Big Data","title":"Big Data","text":"","category":"section"},{"location":"bigdata/#OnlineStats-CSV","page":"Big Data","title":"OnlineStats + CSV","text":"","category":"section"},{"location":"bigdata/","page":"Big Data","title":"Big Data","text":"The CSV package offers a very memory-efficient way of iterating through the rows of a (possibly larger-than-memory) CSV file.","category":"page"},{"location":"bigdata/#Example","page":"Big Data","title":"Example","text":"","category":"section"},{"location":"bigdata/","page":"Big Data","title":"Big Data","text":"Here is a toy example (Iris dataset) of how to iterate through the rows of a CSV file one-by-one and calculate histograms grouped by another variable.","category":"page"},{"location":"bigdata/","page":"Big Data","title":"Big Data","text":"using OnlineStats, CSV, Plots\n\nurl = \"https://gist.githubusercontent.com/joshday/df7bdaa1d58b398592e7656395de6335/raw/5a1c83f498f8ca7e25ff2372340e44b3389be9b1/iris.csv\"\n\nrows = CSV.Rows(download(url); reusebuffer = true)\n\nitr = (string(row.variety) => parse(Float64, row.sepal_length) for row in rows)\n\no = GroupBy(String, Hist(4:0.25:8))\n\nfit!(o, itr)\n\nplot(o, layout=(3,1))","category":"page"},{"location":"bigdata/#Threaded-Parallelism","page":"Big Data","title":"Threaded Parallelism","text":"","category":"section"},{"location":"bigdata/","page":"Big Data","title":"Big Data","text":"The ThreadsX package offers multithreaded implementations of many functions in Base and supports OnlineStats via ThreadsX.reduce(::OnlineStat, data).","category":"page"},{"location":"bigdata/","page":"Big Data","title":"Big Data","text":"See \"A quick introduction to data parallelism in Julia\" by ThreadsX author Takafumi Arakaki (@tkf) for more details.","category":"page"},{"location":"bigdata/#Distributed-Parallelism","page":"Big Data","title":"Distributed Parallelism","text":"","category":"section"},{"location":"bigdata/","page":"Big Data","title":"Big Data","text":"OnlineStats can be merged together to facilitate Embarassingly parallel computations.","category":"page"},{"location":"bigdata/","page":"Big Data","title":"Big Data","text":"note: Note\nIn general, fit! is a cheaper operation than merge!.","category":"page"},{"location":"bigdata/","page":"Big Data","title":"Big Data","text":"warn: Warn\nNot every OnlineStat can be merged.  In these cases, OnlineStats either uses an approximation or provides a warning that no merging occurred.","category":"page"},{"location":"bigdata/#Examples","page":"Big Data","title":"Examples","text":"","category":"section"},{"location":"bigdata/#Simplified-(Not-Actually-in-Parallel)","page":"Big Data","title":"Simplified (Not Actually in Parallel)","text":"","category":"section"},{"location":"bigdata/","page":"Big Data","title":"Big Data","text":"y1 = randn(10_000)\ny2 = randn(10_000)\ny3 = randn(10_000)\n\na = Series(Mean(), Variance(), KHist(20))\nb = Series(Mean(), Variance(), KHist(20))\nc = Series(Mean(), Variance(), KHist(20))\n\nfit!(a, y1)\nfit!(b, y2)\nfit!(c, y3)\n\nmerge!(a, b)  # merge `b` into `a`\nmerge!(a, c)  # merge `c` into `a`","category":"page"},{"location":"bigdata/#In-Parallel","page":"Big Data","title":"In Parallel","text":"","category":"section"},{"location":"bigdata/","page":"Big Data","title":"Big Data","text":"using Distributed\naddprocs(3)\n@everywhere using OnlineStats\n\ns = @distributed merge for i in 1:3\n    o = Series(Mean(), Variance(), KHist(20))\n    fit!(o, randn(10_000))\nend","category":"page"},{"location":"bigdata/","page":"Big Data","title":"Big Data","text":"<img src = \"https://user-images.githubusercontent.com/8075494/57345083-95079780-7117-11e9-81bf-71b0469f04c7.png\" style=\"width:400px\">","category":"page"},{"location":"collections/#Collections-of-Stats","page":"Collections of Stats","title":"Collections of Stats","text":"","category":"section"},{"location":"collections/","page":"Collections of Stats","title":"Collections of Stats","text":"There are a few special OnlineStats that group together other OnlineStats: Series and Group.","category":"page"},{"location":"collections/","page":"Collections of Stats","title":"Collections of Stats","text":"<img src=\"https://user-images.githubusercontent.com/8075494/57342826-bf088c00-710e-11e9-9ac0-f3c1e5aa7a7d.png\" style=\"width:400px\">","category":"page"},{"location":"collections/","page":"Collections of Stats","title":"Collections of Stats","text":"using OnlineStats","category":"page"},{"location":"collections/#Series","page":"Collections of Stats","title":"Series","text":"","category":"section"},{"location":"collections/","page":"Collections of Stats","title":"Collections of Stats","text":"A Series tracks stats that should be applied to the same data stream.","category":"page"},{"location":"collections/","page":"Collections of Stats","title":"Collections of Stats","text":"y = rand(1000)\n\ns = Series(Mean(), Variance())\n\nfit!(s, y)","category":"page"},{"location":"collections/#Group","page":"Collections of Stats","title":"Group","text":"","category":"section"},{"location":"collections/","page":"Collections of Stats","title":"Collections of Stats","text":"A Group tracks stats that should be applied to different data streams.","category":"page"},{"location":"collections/","page":"Collections of Stats","title":"Collections of Stats","text":"g = Group(Mean(), CountMap(Bool))\n\nitr = zip(randn(100), rand(Bool, 100))\n\nfit!(g, itr)","category":"page"},{"location":"","page":"Welcome!","title":"Welcome!","text":"<div style=\"width:100%; height:125px;padding-top:25px;\n        border-radius:6px;text-align:center;background-color:#B3D8FF;\n        color:#000\">\n    <h3 style=\"color: black;\">Star us on GitHub!</h3>\n    <a class=\"github-button\" href=\"https://github.com/joshday/OnlineStats.jl\" data-icon=\"octicon-star\" data-size=\"large\" data-show-count=\"true\" aria-label=\"Star joshday/OnlineStats.jl on GitHub\" style=\"margin:auto\">Star</a>\n    <script async defer src=\"https://buttons.github.io/buttons.js\"></script>\n</div>","category":"page"},{"location":"#Welcome!","page":"Welcome!","title":"Welcome!","text":"","category":"section"},{"location":"","page":"Welcome!","title":"Welcome!","text":"OnlineStats does statistics and data visualization for big/streaming data via online algorithms.  Each algorithm:","category":"page"},{"location":"","page":"Welcome!","title":"Welcome!","text":"processes data one observation at a time.\nuses O(1) memory.","category":"page"},{"location":"#Basics","page":"Welcome!","title":"Basics","text":"","category":"section"},{"location":"#1)-Creating","page":"Welcome!","title":"1) Creating","text":"","category":"section"},{"location":"","page":"Welcome!","title":"Welcome!","text":"Stats are subtypes of OnlineStat{T} where T is the type of a single observation.","category":"page"},{"location":"","page":"Welcome!","title":"Welcome!","text":"using OnlineStats\nm = Mean()\nsupertype(Mean)","category":"page"},{"location":"#2)-Updating","page":"Welcome!","title":"2) Updating","text":"","category":"section"},{"location":"","page":"Welcome!","title":"Welcome!","text":"Stats can be updated with single or multiple observations e.g. fit!(m, 1) and fit!(m, [1,2,3]).","category":"page"},{"location":"","page":"Welcome!","title":"Welcome!","text":"y = randn(100);\nfit!(m, y)\nvalue(m)","category":"page"},{"location":"#3)-Merging","page":"Welcome!","title":"3) Merging","text":"","category":"section"},{"location":"","page":"Welcome!","title":"Welcome!","text":"Stats can be merged.","category":"page"},{"location":"","page":"Welcome!","title":"Welcome!","text":"y2 = randn(100);\nm2 = fit!(Mean(), y2)\nmerge!(m, m2)","category":"page"},{"location":"","page":"Welcome!","title":"Welcome!","text":"warn: Warn\nSome OnlineStats are not analytically mergeable.  In these cases, you will see a warning that either no merging occurred or that the merge is approximate.","category":"page"},{"location":"ml/#Statistical-Learning","page":"Statistical Learning","title":"Statistical Learning","text":"","category":"section"},{"location":"ml/","page":"Statistical Learning","title":"Statistical Learning","text":"The StatLearn (short for statistical learning) OnlineStat uses stochastic approximation methods to estimate models that take the form:","category":"page"},{"location":"ml/","page":"Statistical Learning","title":"Statistical Learning","text":"hatbeta = argmin_beta frac1n sum_i f(y_i x_ibeta) + sum_j lambda_j g(beta_j)","category":"page"},{"location":"ml/","page":"Statistical Learning","title":"Statistical Learning","text":"where","category":"page"},{"location":"ml/","page":"Statistical Learning","title":"Statistical Learning","text":"f is a loss function of a response variable and linear preditor.\nlambda_j's are nonnegative regularization parameters.\ng is a penalty function.","category":"page"},{"location":"ml/","page":"Statistical Learning","title":"Statistical Learning","text":"For example, LASSO Regression fits this form with:","category":"page"},{"location":"ml/","page":"Statistical Learning","title":"Statistical Learning","text":"f(y_i x_ibeta) = frac12(y_i - x_ibeta) ^ 2\ng(beta_j) = beta_j","category":"page"},{"location":"ml/","page":"Statistical Learning","title":"Statistical Learning","text":"OnlineStats implements interchangeable loss and penalty functions to use for both regression and classification problems.  See the StatLearn docstring for details.","category":"page"},{"location":"ml/#Stochastic-Approximation","page":"Statistical Learning","title":"Stochastic Approximation","text":"","category":"section"},{"location":"ml/","page":"Statistical Learning","title":"Statistical Learning","text":"An important note is that StatLearn is unable to estimate coefficients exactly (For precision in regression problems, see LinReg).  The upside is that it makes estimating certain models possible in an online fashion.","category":"page"},{"location":"ml/","page":"Statistical Learning","title":"Statistical Learning","text":"For example, it is not possible to get the same coefficients for logistic regression from an O(1)-memory online algorithm as you would get from its offline counterpart.  This is because the logistic regression likelihood's sufficient statistics scale with the number of observations.","category":"page"},{"location":"ml/","page":"Statistical Learning","title":"Statistical Learning","text":"All this to say: StatLearn lets you do things that would otherwise not be possible at the cost of returning noisy estimates.","category":"page"},{"location":"ml/#Algorithms","page":"Statistical Learning","title":"Algorithms","text":"","category":"section"},{"location":"ml/","page":"Statistical Learning","title":"Statistical Learning","text":"Besides the loss and penalty functions, you can also plug in a variety of fitting algorithms to StatLearn.  Some of these methods are based on the stochastic gradient (gradient of loss evaluated on a single observation).  Other methods are based on the majorization-minimization (MM) principle[1], which offers some guarantees on numerical stability (sometimes at the cost of slower learning).","category":"page"},{"location":"ml/","page":"Statistical Learning","title":"Statistical Learning","text":"[1]: At the moment, the only place to read about the stochastic MM algorithms in detail is Josh Day's dissertation.  Josh is working on an easier-to-digest introduction to these methods and is also happy to discuss them through GitHub issue/email.","category":"page"},{"location":"ml/","page":"Statistical Learning","title":"Statistical Learning","text":"It is a good idea to test out different algorithms on a sample of your dataset.  Plotting the coefficients over time can give you an idea of the stability of the estimates.  Use Trace, a wrapper around an OnlineStat, to automatically take equally-spaced snapshots of an OnlineStat's state.  Keep in mind the early observations will cause bigger jumps in the cofficients than later observations (based on the learning rate; see Weights.  To add further complexity, learning rates (supplied by the rate keyword argument) do not affect each algorithm's learning uniformly.  You may need to test different combinations of algorithm/learning rate to find an \"optimal\" pairing.","category":"page"},{"location":"ml/","page":"Statistical Learning","title":"Statistical Learning","text":"using OnlineStats, Plots\n\n# fake data\nx = rand(Bool, 1000, 10)\ny = x * (1:10) + 10randn(1000)\n\nrate = LearningRate(.8)\n\no = Trace(StatLearn(SGD(), OnlineStats.l2regloss; rate))\no2 = Trace(StatLearn(MSPI(), OnlineStats.l2regloss; rate))\n\nitr = zip(eachrow(x), y)\n\nfit!(o, itr)\nfit!(o2, itr)\n\nplot(\n    plot(o, xlab=\"Nobs\", title=\"SGD Coefficients\", lab=nothing),\n    plot(o2, xlab=\"Nobs\", title=\"MSPI Coefficients\", lab=nothing),\n    link=:y\n)","category":"page"}]
}
